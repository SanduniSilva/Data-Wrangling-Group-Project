<p align="justify"> This report describes the Python data wrangling technique used to clean and prepare five 
interrelated datasets for analysis. Facebook-Google+-and-NewsStory, Missing Publication Date, 
peer-review-patent-and-policy data, Subject codes, and Tweet Data were among the datasets. 
The procedure included dealing with missing data, imputation, reduction, and combining 
datasets that missing publication dates. Enhancing understanding and analytical potential 
required the transformation of the subject codes into subject names. Improved data quality was 
achieved through the use of specialized approaches for smooth integration. The data wrangling 
method included analysis of important factors including structure, granularity, correctness, 
temporality, and scope. For the following study to provide reliable insights, it was essential that 
the datasets meet these quality criteria. 
The article provides an overview of the complex data wrangling process, highlighting the 
challenges that faced and the methods used to enhance data quality. The project is considered 
successful when it results in a single CSV file with consistent headers that is ready for additional 
analysis and decision-making. Researchers and stakeholders looking to maximize the potential 
of these datasets will find the insights gathered from this study to be quite helpful.</p>
